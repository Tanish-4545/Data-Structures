Web Crawling: Breadth First Search (BFS):
Application:Indexing web pages for search engines.
Example: A web crawler uses BFS to visit web pages systematically, starting from a seed URL and exploring links level by level. 
Nodes represent web pages. Edges represent hyperlinks. 
BFS ensures that pages at the same "depth" (distance from the starting page) are visited before moving to deeper levels.
Write a program to simulate the indexing of web pages for a search engine using a Breadth-First Search (BFS) algorithm.

#include<iostream>
using namespace std;
class queue
{
	private:
	int* arr;
	int front;
	int rear;
	int count;
	int maxSize;

	public:
	queue(int n)
	{
		maxSize=n;
		arr=new int[maxSize];
		front=0;
		rear=-1;
		count=0;
	}
	bool isEmpty()
	{
		return (count==0);
	}
	bool isFull()
	{
		return (count==maxSize);
	}
	void enqueue(int val)
	{
		if(isFull())
		{
			cout<<"Queue is full!"<<endl;
			return;
		}
		rear=(rear+1)%maxSize;
		arr[rear]=val;
		count++;
	}
	int getFront()
	{
		if(isEmpty())
		{
			cout<<"Queue is empty!"<<endl;
			return -1;
		}
		return arr[front];
	}
	void pop()
	{
		if(isEmpty())
		{
			cout<<"Queue is empty!"<<endl;
			return;
		}
		front=(front+1)%maxSize;
		count--;
	}
};
void BFS(int p,int n,bool visited[],int** adj)
{
	queue q(n);
	visited[p]=true;
	q.enqueue(p);
	while(!q.isEmpty())
	{
		int curr=q.getFront();
		q.pop();
		cout<<"Current Page is:"<<curr+1<<endl;
		for(int i=0;i<n;i++)
		{
			if(adj[curr][i]==1&&!visited[i])
			{
				visited[i]=true;
				q.enqueue(i);
			}
		}
	}
}
int main()
{
	int n;
	cout<<"Enter Number of Web pages:";
	cin>>n;
	bool* visited=new bool[n];
	int** adj=new int*[n];
	for(int i=0;i<n;i++)
	{
		adj[i]=new int[n];
	}
	for(int i=0;i<n;i++)
	{
		for(int j=0;j<n;j++)
		{
			adj[i][j]=0;
		}
	}
	for(int i=0;i<n;i++)
	{
		visited[i]=0;
	}
	for(int i=0;i<n;i++)
	{
		for(int j=i+1;j<n;j++)
		{
			int ch=0;
			cout<<"If There is Hyperlink from Webpage "<<i+1<<" to "<<j+1<<"\nEnter 1 Else 0:";
			cin>>ch;
			if(ch==1)
			{
				adj[i][j]=ch;
				adj[j][i]=ch;
			}
		}
		cout<<endl;
	}
	for(int i=0;i<n;i++)
	{
		for(int j=0;j<n;j++)
		{
			cout<<adj[i][j]<<" ";
		}
		cout<<endl;
	}
	BFS(0,n,visited,adj);
	return 0;
}

Output:

Depth First Search (DFS):
Application: Web crawlers use DFS to explore web pages systematically, following links and indexing content for search engines. Write a simple program to index web pages using Depth First Search (DFS). The program should simulate a web graph where pages are represented as nodes and hyperlinks as edges.

#include  <iostream>
using namespace std;
void DFS(int p,int n,bool visited[],int** adj){
    visited[p]=1;
    cout<<"Visited Page No. "<<p+1<<endl;
    for(int i=0;i<n;i++){
        if(adj[p][i]==1 && !visited[i]){
            DFS(i,n,visited,adj);
        };
    };
};
int main(){
    int n;
    cout<<"Enter Number of Web pages:";
    cin>>n;
    bool* visited=new bool[n];
    int** adj=new int*[n];
    for (int i=0;i<n;i++){
        adj[i]=new int[n];
    };
        for(int i=0;i<n;i++){
            for(int j=0;j<n;j++){
                adj[i][j]=0;
            };
        };
    for(int i=0;i<n;i++){
        for(int j=i+1;j<n;j++){
            int ch=0;
            cout<<"If There is Hyperlink from Webpage "<<i+1<<" to "<<j+1<<"\nEnter 1 Else 0:";
            cin>>ch;
            if(ch==1){
                adj[i][j]=ch;
                adj[j][i]=ch;
            }
        };
        cout<<endl;
    };
for (int i=0;i<n;i++){
        for (int j=0;j<n;j++){
            cout<<adj[i][j]<<" ";
        };
        cout<<endl;
    };
    for(int i=0;i<n;i++){
        visited[i]=0;
    };
    DFS(0,n,visited,adj);
    return 0;
};

Output:

